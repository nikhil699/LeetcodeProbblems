https://www.linkedin.com/posts/fernando-franco-4696708_if-youre-a-software-engineer-who-want-to-activity-7348715207763869696-7FcN?utm_source=share&utm_medium=member_desktop&rcm=ACoAADEtMvQBOS82FNYVBA08XS2Istc-XujcVQc

How would you design a realtime scalable gaming dashboard? ğŸ®ğŸ“Š

The biggest challenge?

â¡ï¸ Realtime updates â€” As soon as a player's score changes, the system should reflect it immediately.

â¡ï¸ Low-latency aggregations â€” Metrics like total score, number of wins, and time played should be fast to query.

â¡ï¸ Scalability â€” The system should handle millions of users without slowing down.

Can we just use a single database?
Sure â€” it keeps things simple with a single source of truth.
BUT reading and writing from the same DB + running aggregations = ğŸš« not scalable.

How about Master/Replica setup?
Say we use DynamoDB as the main DB and its replica for reads.
Works well for ingestion, but still struggles with aggregations.

âœ… The Solution?
We can leverage Redis for low-latency lookupsâ€¦
â€¦but it's not ideal for complex queries. Let me know which one?

Instead, consider Rockset.
Think of it like a GSI, but optimized for real-time analytics â€” perfect for joins + aggregations + ingestion.

By separating reads from writes, we essentially follow the CQRS pattern â€”
One path for fast ingestion, another for fast querying.

ğŸ’¬ Curious to hear your thoughts â€”
Which DB would you choose for realtime analytics?



ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ­: Screening Round (DP â€“ Hard Level)
- One Dynamic Programming question split into two challenging subparts
- Both parts required complete code solutions with optimal complexity
- Had to provide time and space complexity analysis and justify the approach
- No hand-holding, expected to arrive at edge cases and optimizations independently

ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ®: Coding Round 1 (DP + Binary Search)
2-part DSA round
- Part 1: Dynamic Programming (state design + transitions)
- Part 2: Binary Search on answer space (e.g., â€œMinimize max pagesâ€ type patterns)
- Follow-ups included space optimization and identifying trade-offs
- Heavy emphasis on writing clean, modular code and explaining reasoning in real time

ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ¯: Coding Round 2 (Graph â€“ Multi-Component)
Focused on solving a graph-based problem involving multi-component traversal
- Required identifying and handling connected components using BFS/DFS
- Follow-up involved optimizing traversal logic and handling large-scale input efficiently
Interviewer assessed:
- Clarity in explaining traversal strategy
- Ability to handle edge cases and disconnected graphs
- Clean, modular code and real-time debugging
- Clear verbal walkthrough of approach and decisions


ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ°: System Design + Domain Deep Dive
ğŸ”¹ Part 1: Domain Knowledge & Ownership
Walkthrough of past projects from previous company
Covered:
- Architecture decisions and tech stack rationale
- Cross-team collaboration and stakeholder alignment
- Impact metrics: latency improvements, cost savings, usage growth
- Scenarios highlighting individual contribution vs. team effort
ğŸ”¹ Part 2: System Design Part
Design a Booking service like Book My Show.
- API & Data Model: search shows, reserve/confirm/cancel booking; tables: Shows, Seats, Reservations, Transactions
- Concurrency: optimistic vs. pessimistic locking; prevent double-book
- Throughput: cache show details; rate-limit attempts
- Fault Tolerance: idempotent requests with tokens; auto-release holds on timeout
- Edge Cases: payment partial success; sold-out mid-flow; hold expirations
- Observability: success rate, avg. reservation time; alerts on errors, deadlocks
- Scalability: partition seats by auditorium; horizontal service scaling
- Security: JWT auth; role-based access (admins vs. users)





100 GB customer table is taking 30 seconds to fetch last month's orders (even with proper indexing)

Customers are complaining for slow response times 



Solution: Data Partitioning

Data partitioning divides large tables into smaller, manageable pieces called partitions within a single database server


Benefits

- Each partition contains a subset of your data, allowing the database to access only relevant sections instead of scanning entire tables

- Enables partition-level backup and restore operations

- Minimizes memory usage by loading only required partitions


Types of Partitioning Strategies

1. Range Partitioning -> Divides data based on value ranges (dates, IDs, amounts)

2. Hash Partitioning -> Uses hash functions to distribute data evenly across partitions

3. List Partitioning -> Groups data based on predefined discrete values



ğŸ¤” over to you, which partitioning strategy should we use to split Customer Table?
